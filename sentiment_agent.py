import os
import re
import time as std_time
from datetime import datetime
import requests
import urllib3
from duckduckgo_search import DDGS
from langchain_openai import ChatOpenAI
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ['ALL_PROXY'] = 'socks5://127.0.0.1:7890'
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
URL_SINA_GLOBAL = "https://zhibo.sina.com.cn/api/zhibo/feed?zhibo_id=152&tag_id=0&page=1&page_size=30"
URL_EASTMONEY_NEWS = "https://finance.eastmoney.com/yaowen.html"
URL_10JQKA_REALTIME = "https://news.10jqka.com.cn/tapp/news/push/stock/?page=1&tag=&track=website&pagesize=100"
URL_SINA_ROLL_FUTURES = "https://finance.sina.com.cn/roll/c/56995.shtml"
URL_SINA_HIGHLIGHTS = "https://finance.sina.com.cn/roll/c/56988.shtml"
URL_100PPI = "https://www.100ppi.com/qb/"
URL_MYSTEEL = "https://openapi.mysteel.com/without_sign/newsflash/flashnews/query_by_tags.htm"
URL_WSCN = "https://api-one-wscn.awtmt.com/apiv1/content/lives?channel=global-channel&client=pc&limit=20"
def get_headers(referer="https://www.baidu.com", source="default"):
    base_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    high_version_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0'
    if source == "10jqka":
        return {'User-Agent': high_version_ua, 'Referer': referer, 'X-Requested-With': 'XMLHttpRequest',
                'Accept': 'application/json, text/javascript, */*; q=0.01', 'Connection': 'keep-alive'}
    elif source == "eastmoney":
        return {'User-Agent': high_version_ua, 'Referer': 'https://finance.eastmoney.com/', 'Accept': '*/*',
                'Connection': 'keep-alive'}
    elif source == "sina_html":
        return {'User-Agent': high_version_ua, 'Referer': 'https://finance.sina.com.cn/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Connection': 'keep-alive'}
    elif source == "100ppi":
        return {'User-Agent': high_version_ua, 'Referer': 'https://www.100ppi.com/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Connection': 'keep-alive', 'Cache-Control': 'max-age=0', 'Upgrade-Insecure-Requests': '1'}
    elif source == "mysteel":
        return {'User-Agent': high_version_ua, 'Referer': 'https://www.mysteel.com/', 'Host': 'openapi.mysteel.com',
                'Accept': 'application/json, text/plain, */*', 'Connection': 'keep-alive'}
    elif source == "wscn":
        return {'User-Agent': high_version_ua, 'Referer': 'https://wallstreetcn.com/',
                'Origin': 'https://wallstreetcn.com', 'Connection': 'keep-alive'}
    return {'User-Agent': base_ua, 'Referer': referer, 'Accept': '*/*', 'Connection': 'keep-alive'}
def clean_html(text):
    if not text: return ""
    return re.sub(r'<[^>]+>', '', text).strip()
def find_news_list_recursively(data):
    if isinstance(data, list):
        if len(data) > 0 and isinstance(data[0], dict):
            if any(k in data[0].keys() for k in ['content', 'title', 'digest', 'ctime', 'publishTime']): return data
        for item in data:
            res = find_news_list_recursively(item)
            if res: return res
    elif isinstance(data, dict):
        for key in ['list', 'data', 'items', 'result']:
            if key in data:
                res = find_news_list_recursively(data[key])
                if res: return res
        for value in data.values():
            if isinstance(value, (dict, list)):
                res = find_news_list_recursively(value)
                if res: return res
    return []
# --- å„å¤§ç½‘ç«™çˆ¬è™«æ¨¡å— ---
def fetch_list_sina():
    try:
        resp = requests.get(URL_SINA_GLOBAL, headers=get_headers(), timeout=10, verify=False)
        res = []
        for i in resp.json().get('result', {}).get('data', {}).get('feed', {}).get('list', []):
            # é»˜è®¤å…œåº•æ–°æµª7x24ä¸»é¡µ
            url = i.get('doc_url') or "https://finance.sina.com.cn/7x24/"
            res.append({
                "title": clean_html(i.get('rich_text', ''))[:60],
                "intro": clean_html(i.get('rich_text', '')),
                "source": "æ–°æµªè´¢ç»",
                "url": url,
                "time_ts": int(datetime.strptime(i.get('create_time'), "%Y-%m-%d %H:%M:%S").timestamp())
            })
        return res
    except:
        return []
def fetch_list_10jqka():
    try:
        resp = requests.get(URL_10JQKA_REALTIME, headers=get_headers(source="10jqka"), timeout=10)
        res = []
        for i in find_news_list_recursively(resp.json())[:30]:
            # é»˜è®¤å…œåº•åŒèŠ±é¡ºå¿«è®¯ä¸»é¡µ
            url = i.get('url') or "https://news.10jqka.com.cn/realtimenews/"
            res.append({
                "title": i.get('title', ''),
                "intro": i.get('digest', i.get('title', '')),
                "source": "åŒèŠ±é¡º",
                "url": url,
                "time_ts": int(i.get('ctime', std_time.time()))
            })
        return res
    except:
        return []
def fetch_list_wscn():
    try:
        resp = requests.get(URL_WSCN, headers=get_headers(source="wscn"), timeout=10)
        res = []
        for item in resp.json().get('data', {}).get('items', []):
            content = item.get('content_text', '').strip()
            title = item.get('title', '').strip() or (content[:40] + "..." if content else "")

            raw_uri = item.get('uri', '')
            final_url = ""

            # ã€ä¿®å¤ç‚¹ã€‘ï¼šæ™ºèƒ½è¡¥å…¨åå°”è¡—è§é—»çš„ https å‰ç¼€ï¼Œå»é™¤é”™è¯¯çš„ Markdown æ ¼å¼
            if raw_uri:
                final_url = raw_uri if raw_uri.startswith("http") else f"https://wallstreetcn.com{raw_uri}"

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æ–‡ç« çš„é“¾æ¥
            ref_article = item.get('reference_article')
            if ref_article and isinstance(ref_article, dict) and ref_article.get('uri'):
                ref_uri = ref_article.get('uri')
                final_url = ref_uri if ref_uri.startswith("http") else f"https://wallstreetcn.com{ref_uri}"

            if not final_url and item.get('id'):
                final_url = f"https://wallstreetcn.com/live/global/{item.get('id')}"

            res.append({
                "title": title,
                "intro": content or title,
                "source": "åå°”è¡—è§é—»",
                "url": final_url,
                "time_ts": item.get('display_time', int(std_time.time()))
            })
        return res
    except:
        return []
def fetch_list_eastmoney():
    try:
        headers = get_headers(source="eastmoney")
        resp = requests.get(URL_EASTMONEY_NEWS, headers=headers, timeout=10, verify=False)
        resp.encoding = 'utf-8'
        pattern = re.compile(
            r'<p class="title"[^>]*>.*?<a\s+[^>]*href="(https?://finance\.eastmoney\.com/a/[^"]+)"[^>]*>(.*?)</a>.*?<p class="time">\s*(\d{1,2}æœˆ\d{1,2}æ—¥\s+\d{2}:\d{2})\s*</p>',
            re.S
        )
        matches = list(pattern.finditer(resp.text))
        res = []
        current_year = datetime.now().year
        for match in matches:
            title = clean_html(match.group(2)).strip()
            time_str = match.group(3)
            try:
                ts = int(datetime.strptime(f"{current_year}å¹´{time_str}", "%Yå¹´%mæœˆ%dæ—¥ %H:%M").timestamp())
            except:
                ts = int(std_time.time())
            res.append({"title": title, "intro": title, "url": match.group(1), "time_ts": ts, "source": "ä¸œè´¢è¦é—»"})
        return res[:30]
    except Exception as e:
        return []
def parse_sina_roll_page(url, source_name):
    try:
        resp = requests.get(url, headers=get_headers(source="sina_html"), timeout=10)
        resp.encoding = resp.apparent_encoding if resp.apparent_encoding else 'gbk'
        pattern = re.compile(
            r'<a\s+href="([^"]+)"[^>]*target="_blank">([^<]+)</a>.*?\((\d{2}æœˆ\d{2}æ—¥\s+\d{2}:\d{2})\)', re.S)
        matches = pattern.findall(resp.text)
        res = []
        current_year = datetime.now().year
        for url_path, title, time_str in matches[:30]:
            try:
                ts = int(datetime.strptime(f"{current_year}å¹´{time_str}", "%Yå¹´%mæœˆ%dæ—¥ %H:%M").timestamp())
            except:
                ts = int(std_time.time())
            res.append(
                {"title": title.strip(), "intro": title.strip(), "url": url_path, "time_ts": ts, "source": source_name})
        return res
    except:
        return []
def fetch_list_100ppi():
    try:
        t_param = int(std_time.time() * 1000)
        target_url = f"{URL_100PPI}?_t={t_param}"
        resp = requests.get(target_url, headers=get_headers(source="100ppi"), timeout=15, verify=False)
        html = resp.content.decode('utf-8', errors='ignore')
        item_pattern = re.compile(r'(\d{2}:\d{2}).*?<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', re.S)
        matches = list(item_pattern.finditer(html))
        res = []
        today_str = datetime.now().strftime("%Y-%m-%d")
        for i, match in enumerate(matches):
            try:
                time_str = match.group(1)
                initial_url = match.group(2)
                raw_title_html = match.group(3)
                start_pos = match.end()
                end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(html)
                content_block = html[start_pos:end_pos]

                # è·å–ç‚¹å‡»è¯¦æƒ…çš„çœŸå®é“¾æ¥
                detail_match = re.search(r'href="([^"]+)"[^>]*>\s*ç‚¹å‡»è¯¦æƒ…', content_block)
                final_url = detail_match.group(1) if detail_match else initial_url

                # ã€ä¿®å¤ç‚¹ã€‘ï¼šä¸ºç”Ÿæ„ç¤¾çš„æ®‹ç¼ºç›¸å¯¹è·¯å¾„å¼ºåˆ¶è¡¥å…¨ https å‰ç¼€
                if not final_url.startswith("http"):
                    final_url = f"https://www.100ppi.com{final_url}" if final_url.startswith(
                        "/") else f"https://www.100ppi.com/{final_url}"

                title_text = clean_html(raw_title_html).strip()
                summary_text = clean_html(content_block).replace("ç‚¹å‡»è¯¦æƒ…", "").strip()
                intro = summary_text if len(summary_text) > 5 else title_text
                try:
                    ts = int(datetime.strptime(f"{today_str} {time_str}:00", "%Y-%m-%d %H:%M:%S").timestamp())
                except:
                    ts = int(std_time.time())
                res.append({"title": title_text, "intro": intro, "url": final_url, "time_ts": ts, "source": "ç”Ÿæ„ç¤¾"})
            except:
                continue
        return res
    except:
        return []
def fetch_list_mysteel():
    try:
        params = {"advertisementFlag": "0", "keyword": "", "pageNo": "1", "pageSize": "30", "sortByScore": "false",
                  "columnIds": "[[2,84,584]]"}
        resp = requests.get(URL_MYSTEEL, headers=get_headers(source="mysteel"), params=params, timeout=10)
        items = find_news_list_recursively(resp.json())
        res = []
        for item in items[:30]:
            ts_ms = item.get('publishTime') or item.get('createTime') or 0
            ts = int(ts_ms / 1000) if ts_ms > 10000000000 else int(std_time.time())

            link = item.get('linkUrl') or item.get('wapUrl') or ""
            if not link and 'id' in item:
                link = f"https://news.mysteel.com/{item['id']}.html"

            # ã€ä¿®å¤ç‚¹ã€‘ï¼šå¤åŸäº†ä½ åŸæœ¬åç«¯é‡Œçš„é»˜è®¤è·³è½¬å…œåº•é€»è¾‘
            if not link:
                link = "https://www.mysteel.com/fastcomment/#/"

            res.append({"title": item.get('title', '').strip(),
                        "intro": clean_html(item.get('content', '') or item.get('summary', '')), "url": link,
                        "time_ts": ts, "source": "æˆ‘çš„é’¢é“"})
        return res
    except:
        return []
# ==========================================
# 3. å®šå‘æœç´¢ä¸æ·±åº¦æŠ“å– (DuckSearch)
# ==========================================
def fetch_url_content_realtime(url, source="default"):
    if not url or not url.startswith("http"): return ""
    try:
        resp = requests.get(url, headers=get_headers(url, source=source), timeout=10, verify=False)
        resp.encoding = resp.apparent_encoding or 'utf-8'
        paragraphs = re.findall(r'<p.*?>(.*?)</p>', resp.text, re.S)
        return "\n".join([clean_html(p) for p in paragraphs if len(clean_html(p)) > 10])[:800]
    except:
        return ""
def search_web_context(query, max_results=5):
    """ä½¿ç”¨ DuckDuckGo è¿›è¡Œå…¨ç½‘å®šå‘æœç´¢"""
    print(f"   [DuckSearch] æ­£åœ¨å…¨ç½‘æ£€ç´¢å…³é”®è¯: '{query}'")
    results = []
    try:
        with DDGS(timeout=15) as ddgs:
            results = list(ddgs.text(query, region='wt-wt', safesearch='off', timelimit='w', max_results=max_results))
    except Exception as e:
        print(f"   âš ï¸ æœç´¢è­¦å‘Š: {e}")

    if not results:
        return "ï¼ˆæœªæœåˆ°è¿‘æœŸå¼ºç›¸å…³çš„å®šå‘èµ„è®¯ï¼‰",[]

    context_str = ""
    raw_news_list = []
    for i, res in enumerate(results):
        # æå–ç›¸å…³å­—æ®µï¼Œæ³¨æ„ DuckDuckGo è¿”å›çš„é“¾æ¥å­—æ®µé€šå¸¸æ˜¯ 'href'
        title = res.get('title', 'æœªçŸ¥æ ‡é¢˜')
        url = res.get('href', '#')  # æŠ“å–åŸç½‘å€
        body = res.get('body', '')
        published = res.get('published', 'è¿‘æœŸ')

        context_str += f"[{i + 1}] æ¥æº: {title}\n    æ—¶é—´: {published}\n    å†…å®¹: {body}\n\n"

        # æ–°å¢ï¼šå°†å­—å…¸å­˜å…¥åˆ—è¡¨
        raw_news_list.append({
            "title": title,
            "url": url,
            "content": body,
            "time": published
        })
    return context_str, raw_news_list
# ==========================================
# 4. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘
# ==========================================
def llm_pre_process_news(raw_text_block, ticker, llm):
    """
    ç¬¬ä¸€é˜¶æ®µï¼šåˆ©ç”¨å¤§æ¨¡å‹å¯¹åŸå§‹æ–°é—»è¿›è¡Œè¯­ä¹‰çº§çš„å»å™ªå’Œæ ‡ç­¾åŒ–
    """
    print(f"   [è¯­ä¹‰è¿‡æ»¤] æ­£åœ¨è®©å¤§æ¨¡å‹å¯¹ {ticker} çš„èµ„è®¯è¿›è¡Œåˆæ­¥é‰´å®šä¸æ ‡ç­¾åŒ–...")

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ— æƒ…çš„æ–°é—»è¿‡æ»¤å™¨ã€‚ä»¥ä¸‹æ˜¯æŠ“å–åˆ°çš„å…³äº {ticker} çš„åŸå§‹æ‚ä¹±èµ„è®¯ï¼š

    {raw_text_block}

    ã€ä½ çš„ä»»åŠ¡ã€‘ï¼š
    1. å‰”é™¤æ‰€æœ‰ä¸ {ticker} æ ¸å¿ƒåŸºæœ¬é¢/è‚¡ä»·æ— å…³çš„å™ªéŸ³ï¼ˆå¦‚æ—¥å¸¸é—²èŠã€ä¸ç›¸å¹²çš„å¹¿å‘Šã€æ— æ„ä¹‰çš„é‡å¤é€šç¨¿ï¼‰ã€‚
    2. å¯¹ä¿ç•™ä¸‹æ¥çš„é«˜ä»·å€¼æ–°é—»è¿›è¡Œå‹ç¼©ï¼Œå¹¶æ‰“ä¸Šæ ‡ç­¾ã€‚å¯é€‰æ ‡ç­¾ï¼š[æ”¿ç­–å®è§‚]ã€[è´¢æŠ¥ä¸šç»©]ã€[çªå‘äº‹ä»¶]ã€[å¸‚åœºæƒ…ç»ª]ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
    è¯·ç›´æ¥è¾“å‡ºä¿ç•™ä¸‹æ¥çš„é«˜ä»·å€¼æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡ä¸€è¡Œï¼Œæ ¼å¼å¿…é¡»ä¸ºï¼š
    æ ‡ç­¾ï¼š[xxx] | æ ¸å¿ƒäº‹ä»¶ï¼š(ç”¨ä¸€å¥è¯æ¦‚æ‹¬) | æ½œåœ¨å½±å“ï¼š(åå¤š/åç©º/ä¸­æ€§)
    å¦‚æœå…¨éƒ¨éƒ½æ˜¯å™ªéŸ³ï¼Œè¯·å›å¤â€œæ— é«˜ä»·å€¼å¢é‡ä¿¡æ¯â€ã€‚
    """

    # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œé¢„å¤„ç†
    response = llm.invoke(prompt)
    return response.content
def filter_and_clean_news(raw_news_list, company_keyword=None, max_count=10):
    """
    ã€æ•°æ®æ¸…æ´—ç®¡é“ã€‘è´Ÿè´£è¿‡æ»¤çˆ¬è™«æŠ“å–åˆ°çš„å™ªéŸ³æ•°æ®ï¼Œå¹¶ä¿ç•™æ—¶é—´å­—æ®µ
    """
    cleaned_news = []
    seen_titles = set()

    for news in raw_news_list:
        title = news.get("title", "") or news.get("name", "")
        content = news.get("content", "") or news.get("snippet", "") or news.get("intro", "")
        url = news.get("url", "") or news.get("link", "")
        source = news.get("source", "ç½‘ç»œ")

        # --- æ–°å¢ï¼šæå–å¹¶æ ¼å¼åŒ–æ—¶é—´ ---
        raw_time = news.get("time") or news.get("time_ts")
        if isinstance(raw_time, int):
            # å¦‚æœæ˜¯æ—¶é—´æˆ³ï¼Œè½¬ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²æ ¼å¼
            from datetime import datetime
            time_str = datetime.fromtimestamp(raw_time).strftime('%Y-%m-%d %H:%M')
        else:
            time_str = raw_time or "è¿‘æœŸ"

        if not title or title in seen_titles:
            continue
        if len(content) < 15 and len(title) < 5:
            continue
        if company_keyword and (company_keyword not in title and company_keyword not in content):
            continue

        seen_titles.add(title)

        # ç»Ÿä¸€æ ‡å‡†åŒ–æ ¼å¼å¹¶ä¿ç•™æºé“¾æ¥ï¼ˆæ–°å¢ time å­—æ®µï¼‰
        cleaned_news.append({
            "title": title,
            "content": content,
            "url": url,
            "source": source,
            "time": time_str  # è§£å†³å‰ç«¯ undefined çš„é—®é¢˜
        })

        if len(cleaned_news) >= max_count:
            break

    return cleaned_news
def run_sentiment_agent(ticker: str, api_key: str) -> dict:
    print(f"\n[æƒ…ç»ªç»„] æ­£åœ¨å…¨ç½‘æœé›† {ticker} çš„æ–°é—»èµ„è®¯ä¸æ•£æˆ·èˆ†æƒ…...")
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url="https://api.deepseek.com",
        temperature=0.3
    )

    # ==========================================
    # æ­¥éª¤ A: æŠ“å–å…¨ç½‘æœ€æ–°å®è§‚å¿«è®¯
    # ==========================================
    print("   [çˆ¬è™«é›†ç¾¤] æ­£åœ¨æ‹‰å– æ–°æµª/åŒèŠ±é¡º/åå°”è¡—è§é—» å®æ—¶æ»šåŠ¨å¿«è®¯...")
    raw_macro_news = (
            fetch_list_sina() +
            fetch_list_eastmoney() +
            fetch_list_10jqka() +
            parse_sina_roll_page(URL_SINA_ROLL_FUTURES, "æ–°æµªæœŸè´§-æ»šåŠ¨") +
            parse_sina_roll_page(URL_SINA_HIGHLIGHTS, "æ–°æµªæœŸè´§-è¦é—»") +
            fetch_list_100ppi() +
            fetch_list_mysteel() +
            fetch_list_wscn()
    )
    # å‡è®¾ä½ çš„åŸå§‹æ•°æ®é‡Œæœ‰ time_ts è¿™ä¸ªå­—æ®µç”¨äºæ’åº
    raw_macro_news.sort(key=lambda x: x.get('time_ts', 0), reverse=True)

    # æ¸…æ´—å®è§‚æ–°é—» (ä¸éœ€è¦ä¼ å…¥ ticker ä½œä¸ºè¿‡æ»¤è¯ï¼Œä»¥ä¿ç•™å¤§ç›˜æ”¿ç­–ä¿¡æ¯)
    cleaned_macro = filter_and_clean_news(raw_macro_news, max_count=15)
    macro_text = ""
    for item in cleaned_macro:
        macro_text += f"[{item['source']}] {item['title']}\n"

    # ==========================================
    # æ­¥éª¤ B: é’ˆå¯¹æ ‡çš„å®šå‘æœç´¢ (DuckDuckGo)
    # ==========================================
    stock_code = ticker.split('.')[-1] if '.' in ticker else ticker
    search_query = f"{stock_code} è‚¡ç¥¨ çªå‘ æœ€æ–°æ¶ˆæ¯ æ¶¨è·ŒåŸå› "
    print(f"   [å®šå‘æœç´¢] æ­£åœ¨é€šè¿‡ DuckDuckGo æ·±åº¦æŒ–æ˜: {search_query}")

    # å‡è®¾ search_web_context è¿”å› (çº¯æ–‡æœ¬æ‘˜è¦, åŸå§‹å­—å…¸åˆ—è¡¨)
    _, raw_specific_news = search_web_context(search_query, max_results=8)

    # æ¸…æ´—å¾®è§‚æƒ…æŠ¥
    # æ¸…æ´—å¾®è§‚æƒ…æŠ¥
    cleaned_specific = filter_and_clean_news(raw_specific_news, max_count=8)
    specific_news_text = ""
    for i, news in enumerate(cleaned_specific, 1):
        specific_news_text += f"ã€æ–°é—» {i}ã€‘æ ‡é¢˜ï¼š{news['title']}\næ‘˜è¦ï¼š{news['content']}\n\n"

    # ==========================================
    # æ­¥éª¤ B.5: ç»„è£…å±•ç¤ºç»™å‰ç«¯çš„æ–°é—»åˆ—è¡¨å¹¶å¢åŠ æ ‡è¯†
    # (æ³¨æ„ï¼šè¿™æ®µä»£ç å¿…é¡»åœ¨ for å¾ªç¯çš„å¤–é¢ï¼ï¼ï¼)
    # ==========================================
    # 1. å¦‚æœå¾®è§‚ï¼ˆä¸ªè‚¡ï¼‰æ–°é—»æ²¡æœåˆ°ï¼ˆå¦‚ç½‘ç»œè¶…æ—¶ï¼‰ï¼Œå¢åŠ ä¸€æ¡ç³»ç»Ÿæç¤º
    if not cleaned_specific:
        cleaned_specific = [{
            "title": f"âš ï¸ æœªæ£€ç´¢åˆ° {ticker} çš„å®šå‘å¾®è§‚æ–°é—»",
            "content": "ä¸ªè‚¡å®šå‘æœç´¢æ¥å£å¯èƒ½è¶…æ—¶æˆ–è¢«æ‹¦æˆªã€‚ä»¥ä¸‹ä¸ºæ‚¨å±•ç¤ºå…¨ç½‘æœ€æ–°çš„å®è§‚å¤§ç›˜ä¸è¡Œä¸šå¿«è®¯ï¼Œä½œä¸ºå¸‚åœºç¯å¢ƒå‚è€ƒã€‚",
            "url": "javascript:void(0);",
            "source": "ç³»ç»Ÿæç¤º",
            "time": datetime.now().strftime('%Y-%m-%d %H:%M')
        }]

    # 2. ç»™å®è§‚æ–°é—»åŠ ä¸Šé†’ç›®çš„å‰ç¼€ï¼Œé˜²æ­¢è¯¯ä¼š
    for item in cleaned_macro:
        if not item['title'].startswith("ã€å®è§‚å¤§åŠ¿ã€‘"):
            item['title'] = "ã€å®è§‚å¤§åŠ¿ã€‘" + item['title']

    # åˆå¹¶é«˜è´¨é‡æ–°é—»ï¼Œç”¨äºè¿”å›ç»™å‰ç«¯å±•ç¤ºå¯ç‚¹å‡»çš„é“¾æ¥
    final_news_links = cleaned_specific + cleaned_macro[:5]
    # ==========================================
    # æ­¥éª¤ C: è°ƒç”¨ DeepSeek åˆ†ææƒ…ç»ª
    # ==========================================
    print("   [å¤§è„‘æ€è€ƒ] æ­£åœ¨ç»¼åˆå¾®è§‚ä¸ªè‚¡ä¸å®è§‚å¤§åŠ¿ï¼Œè¯„ä¼°å¸‚åœºæƒ…ç»ª...")

    # ã€å…¨é¢å‡çº§çš„ Promptï¼šæ—¢æœ‰å®è§‚ï¼Œåˆæœ‰å¾®è§‚ã€‘
    n_data = f"""
        ã€1. æ ‡çš„å®šå‘å¾®è§‚æƒ…æŠ¥ã€‘:
        {specific_news_text}

        ã€2. å½“å‰å¤§ç›˜å®è§‚èƒŒæ™¯ã€‘:
        {macro_text}
        """
    cleaned_and_tagged_news = llm_pre_process_news(n_data, ticker, llm)

    print(f"   [è¿‡æ»¤å®Œæˆ] æç‚¼å‡ºçš„é«˜ä»·å€¼æ ‡ç­¾åŒ–æƒ…æŠ¥å¦‚ä¸‹ï¼š\n{cleaned_and_tagged_news}")

    # ==========================================
    # æ­¥éª¤ C: è°ƒç”¨ DeepSeek åˆ†ææƒ…ç»ª (ä¿®æ”¹æœ€ç»ˆçš„ Prompt)
    # ==========================================
    print("   [å¤§è„‘æ€è€ƒ] æ­£åœ¨åŸºäºé«˜çº¯åº¦æƒ…æŠ¥è¯„ä¼°æœ€ç»ˆå¸‚åœºæƒ…ç»ª...")

    # ã€ä¿®æ”¹ç‚¹ã€‘ï¼šå°†æœ€ç»ˆçš„ Prompt æ›¿æ¢æˆå–‚ç»™å®ƒâ€œæ´—å¥½çš„æ ‡ç­¾åŒ–æ•°æ®â€
    prompt = f"""
        ä½ æ˜¯èµ„æ·±çš„é‡åŒ–å¯¹å†²åŸºé‡‘èˆ†æƒ…åˆ†æå¸ˆã€‚
        ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ æä¾›ç»è¿‡ä¸¥æ ¼é¢„æ¸…æ´—å’Œæ ‡ç­¾åŒ–çš„å…³äºæ ‡çš„ {ticker} çš„æ ¸å¿ƒé«˜ä»·å€¼æƒ…æŠ¥ï¼š

        {cleaned_and_tagged_news}
    ã€ä»»åŠ¡è¦æ±‚ã€‘
    1. ç»“åˆå®è§‚å¤§åŠ¿å’Œå¾®è§‚ä¸ªè‚¡æƒ…æŠ¥ï¼Œåˆ†ææ•´ä½“æƒ…ç»ªã€‚
    2. å¿½ç•¥ä¸­æ€§çš„æ—¥å¸¸å…¬å‘Šï¼Œè¯·åƒçŒçŠ¬ä¸€æ ·ç²¾å‡†å®šä½å¯¹è‚¡ä»·æœ‰å®è´¨æ€§å½±å“çš„äº‹ä»¶ã€‚

    ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘ï¼ˆè¯·ä¸¥æ ¼æŒ‰æ­¤æ ¼å¼è¾“å‡ºï¼‰
    ã€æƒ…ç»ªè§‚ç‚¹ã€‘çœ‹æ¶¨/çœ‹è·Œ/éœ‡è¡ä¸­æ€§
    ã€æ ¸å¿ƒé©±åŠ¨äº‹ä»¶ã€‘(æå–æœ€å…·å½±å“åŠ›çš„å…³é”®æ–°é—»ï¼Œå¹¶ç®€è¿°å…¶æ½œåœ¨çš„çœ‹å¤š/çœ‹ç©ºé€»è¾‘)
    ã€æ½œåœ¨æƒ…ç»ªé£é™©ã€‘(å½“å‰æ˜¯å¦å­˜åœ¨èˆ†è®ºä¸Šçš„éšæ‚£ã€â€œä¹°é¢„æœŸï¼Œå–äº‹å®â€çš„é£é™©æˆ–ç³»ç»Ÿæ€§å¤§ç›˜é£é™©ï¼Ÿ)
    """

    response = llm.invoke(prompt)

    return {
        "news_data": specific_news_text + "\n" + macro_text,
        "news_links": final_news_links,
        "sentiment_signal": response.content
    }
# ==========================================
# 5. ç‹¬ç«‹æµ‹è¯•å…¥å£
# ==========================================
if __name__ == "__main__":
    test_ticker = "sh.600519"  # æµ‹è¯•æ ‡çš„ï¼šè´µå·èŒ…å°
    print(f"ğŸš€ å¯åŠ¨ã€å…¨ç½‘ç‰ˆã€‘å¸‚åœºæƒ…ç»ªæ™ºèƒ½ä½“ç‹¬ç«‹æµ‹è¯• (ç›®æ ‡: {test_ticker})")
    print("-" * 50)

    result = run_sentiment_agent(test_ticker, "sk-xxxxxxè¿™é‡Œæ¢æˆä½ çš„çœŸå®key")

    print("\n" + "=" * 50)
    print("ğŸŒ æœé›†åˆ°çš„åŸå§‹èˆ†æƒ…ä¸çˆ¬è™«èšåˆæ•°æ®:")
    print(result["news_data"])
    print("\n" + "=" * 50)
    print("ğŸ§  æ™ºèƒ½ä½“æƒ…ç»ªåˆ†ææŠ¥å‘Š:")
    print(result["sentiment_signal"])
    print("=" * 50)