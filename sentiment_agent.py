import os
import re
import time as std_time
from datetime import datetime
import requests
import urllib3
from duckduckgo_search import DDGS
from langchain_openai import ChatOpenAI

# ç¦ç”¨ HTTPS è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# æŠ“å–æºå¸¸é‡é…ç½®
URL_SINA_GLOBAL = "https://zhibo.sina.com.cn/api/zhibo/feed?zhibo_id=152&tag_id=0&page=1&page_size=30"
URL_EASTMONEY_NEWS = "https://finance.eastmoney.com/yaowen.html"
URL_10JQKA_REALTIME = "https://news.10jqka.com.cn/tapp/news/push/stock/?page=1&tag=&track=website&pagesize=100"
URL_SINA_ROLL_FUTURES = "https://finance.sina.com.cn/roll/c/56995.shtml"
URL_SINA_HIGHLIGHTS = "https://finance.sina.com.cn/roll/c/56988.shtml"
URL_100PPI = "https://www.100ppi.com/qb/"
URL_MYSTEEL = "https://openapi.mysteel.com/without_sign/newsflash/flashnews/query_by_tags.htm"
URL_WSCN = "https://api-one-wscn.awtmt.com/apiv1/content/lives?channel=global-channel&client=pc&limit=20"


# ==========================================
# 2. çˆ¬è™«å·¥å…·å‡½æ•° (å®Œç¾ç»§æ‰¿ä½ çš„ Backend é€»è¾‘)
# ==========================================
def get_headers(referer="https://www.baidu.com", source="default"):
    base_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    high_version_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0'
    if source == "10jqka":
        return {'User-Agent': high_version_ua, 'Referer': referer, 'X-Requested-With': 'XMLHttpRequest',
                'Accept': 'application/json, text/javascript, */*; q=0.01', 'Connection': 'keep-alive'}
    elif source == "eastmoney":
        return {'User-Agent': high_version_ua, 'Referer': 'https://finance.eastmoney.com/', 'Accept': '*/*',
                'Connection': 'keep-alive'}
    elif source == "sina_html":
        return {'User-Agent': high_version_ua, 'Referer': 'https://finance.sina.com.cn/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Connection': 'keep-alive'}
    elif source == "100ppi":
        return {'User-Agent': high_version_ua, 'Referer': 'https://www.100ppi.com/', 'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1'}
    elif source == "wscn":
        return {'User-Agent': high_version_ua, 'Referer': 'https://wallstreetcn.com/',
                'Origin': 'https://wallstreetcn.com', 'Connection': 'keep-alive'}
    return {'User-Agent': base_ua, 'Referer': referer, 'Accept': '*/*', 'Connection': 'keep-alive'}


def clean_html(text):
    if not text: return ""
    return re.sub(r'<[^>]+>', '', text).strip()


def find_news_list_recursively(data):
    if isinstance(data, list):
        if len(data) > 0 and isinstance(data[0], dict):
            if any(k in data[0].keys() for k in ['content', 'title', 'digest', 'ctime', 'publishTime']): return data
        for item in data:
            res = find_news_list_recursively(item)
            if res: return res
    elif isinstance(data, dict):
        for key in ['list', 'data', 'items', 'result']:
            if key in data:
                res = find_news_list_recursively(data[key])
                if res: return res
        for value in data.values():
            if isinstance(value, (dict, list)):
                res = find_news_list_recursively(value)
                if res: return res
    return []


# --- å„å¤§ç½‘ç«™çˆ¬è™«æ¨¡å— ---
def fetch_list_sina():
    try:
        resp = requests.get(URL_SINA_GLOBAL, headers=get_headers(), timeout=10, verify=False)
        return [{"title": clean_html(i.get('rich_text', ''))[:60], "intro": clean_html(i.get('rich_text', '')),
                 "source": "æ–°æµªè´¢ç»",
                 "time_ts": int(datetime.strptime(i.get('create_time'), "%Y-%m-%d %H:%M:%S").timestamp())} for i in
                resp.json().get('result', {}).get('data', {}).get('feed', {}).get('list', [])]
    except:
        return []


def fetch_list_10jqka():
    try:
        resp = requests.get(URL_10JQKA_REALTIME, headers=get_headers(source="10jqka"), timeout=10)
        return [{"title": i.get('title', ''), "intro": i.get('digest', i.get('title', '')), "source": "åŒèŠ±é¡º",
                 "time_ts": int(i.get('ctime', std_time.time()))} for i in find_news_list_recursively(resp.json())[:30]]
    except:
        return []


def fetch_list_wscn():
    try:
        resp = requests.get(URL_WSCN, headers=get_headers(source="wscn"), timeout=10)
        res = []
        for item in resp.json().get('data', {}).get('items', []):
            content = item.get('content_text', '').strip()
            title = item.get('title', '').strip() or (content[:40] + "..." if content else "")
            res.append({"title": title, "intro": content or title, "source": "åå°”è¡—è§é—»",
                        "time_ts": item.get('display_time', int(std_time.time()))})
        return res
    except:
        return []


# ==========================================
# 3. å®šå‘æœç´¢ä¸æ·±åº¦æŠ“å– (DuckSearch)
# ==========================================
def fetch_url_content_realtime(url, source="default"):
    if not url or not url.startswith("http"): return ""
    try:
        resp = requests.get(url, headers=get_headers(url, source=source), timeout=10, verify=False)
        resp.encoding = resp.apparent_encoding or 'utf-8'
        paragraphs = re.findall(r'<p.*?>(.*?)</p>', resp.text, re.S)
        return "\n".join([clean_html(p) for p in paragraphs if len(clean_html(p)) > 10])[:800]
    except:
        return ""


def search_web_context(query, max_results=5):
    """ä½¿ç”¨ DuckDuckGo è¿›è¡Œå…¨ç½‘å®šå‘æœç´¢"""
    print(f"   [DuckSearch] æ­£åœ¨å…¨ç½‘æ£€ç´¢å…³é”®è¯: '{query}'")
    results = []
    try:
        with DDGS(timeout=15) as ddgs:
            results = list(ddgs.text(query, region='wt-wt', safesearch='off', timelimit='w', max_results=max_results))
    except Exception as e:
        print(f"   âš ï¸ æœç´¢è­¦å‘Š: {e}")

    if not results:
        return "ï¼ˆæœªæœåˆ°è¿‘æœŸå¼ºç›¸å…³çš„å®šå‘èµ„è®¯ï¼‰",[]

    context_str = ""
    raw_news_list = []
    for i, res in enumerate(results):
        # æå–ç›¸å…³å­—æ®µï¼Œæ³¨æ„ DuckDuckGo è¿”å›çš„é“¾æ¥å­—æ®µé€šå¸¸æ˜¯ 'href'
        title = res.get('title', 'æœªçŸ¥æ ‡é¢˜')
        url = res.get('href', '#')  # æŠ“å–åŸç½‘å€
        body = res.get('body', '')
        published = res.get('published', 'è¿‘æœŸ')

        context_str += f"[{i + 1}] æ¥æº: {title}\n    æ—¶é—´: {published}\n    å†…å®¹: {body}\n\n"

        # æ–°å¢ï¼šå°†å­—å…¸å­˜å…¥åˆ—è¡¨
        raw_news_list.append({
            "title": title,
            "url": url,
            "content": body,
            "time": published
        })
    return context_str, raw_news_list


# ==========================================
# 4. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘
# ==========================================
def run_sentiment_agent(ticker: str,api_key:str) -> dict:
    print(f"\n[æƒ…ç»ªç»„] æ­£åœ¨å…¨ç½‘æœé›† {ticker} çš„æ–°é—»èµ„è®¯ä¸æ•£æˆ·èˆ†æƒ…...")
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,  # ä½¿ç”¨å‚æ•°ä¼ è¿‡æ¥çš„ key
        base_url="https://api.deepseek.com",
        temperature=0.3  # ï¼ˆæ³¨æ„ï¼šå„ä¸ªagentåŸæœ‰çš„æ¸©åº¦ä¿ç•™ä¸å˜ï¼Œæ¯”å¦‚riskæ˜¯0.1ï¼‰
    )
    # æ­¥éª¤ A: æŠ“å–å…¨ç½‘æœ€æ–°å®è§‚å¿«è®¯
    print("   [çˆ¬è™«é›†ç¾¤] æ­£åœ¨æ‹‰å– æ–°æµª/åŒèŠ±é¡º/åå°”è¡—è§é—» å®æ—¶æ»šåŠ¨å¿«è®¯...")
    raw_news = fetch_list_sina() + fetch_list_10jqka() + fetch_list_wscn()
    raw_news.sort(key=lambda x: x['time_ts'], reverse=True)

    # å»é‡å¹¶æå–å‰ 15 æ¡ä½œä¸ºå®è§‚èƒŒæ™¯
    macro_background = []
    seen = set()
    for item in raw_news:
        if item['title'] not in seen:
            seen.add(item['title'])
            macro_background.append(f"[{item['source']}] {item['title']}")
        if len(macro_background) >= 15: break

    macro_text = "\n".join(macro_background)

    # æ­¥éª¤ B: é’ˆå¯¹æ ‡çš„å®šå‘æœç´¢ (ä½¿ç”¨ DuckDuckGo)
    stock_code = ticker.split('.')[-1] if '.' in ticker else ticker
    search_query = f"{stock_code} è‚¡ç¥¨ çªå‘ æœ€æ–°æ¶ˆæ¯ æ¶¨è·ŒåŸå› "
    specific_news_text, raw_news_list = search_web_context(search_query, max_results=4)
    n_data = f"""
    ã€1. æ ‡çš„å®šå‘å¾®è§‚æƒ…æŠ¥ (DuckDuckGo æœç´¢)ã€‘:
    {specific_news_text}

    ã€2. å½“å‰å¤§ç›˜å®è§‚èƒŒæ™¯ (7x24å°æ—¶å…¨ç½‘è´¢ç»å¿«è®¯)ã€‘:
    {macro_text}
    """

    # æ­¥éª¤ C: è°ƒç”¨ DeepSeek åˆ†ææƒ…ç»ª
    print("   [å¤§è„‘æ€è€ƒ] æ­£åœ¨ç»¼åˆå¾®è§‚ä¸ªè‚¡ä¸å®è§‚å¤§åŠ¿ï¼Œè¯„ä¼°å¸‚åœºæƒ…ç»ª...")
    prompt = f"""
    ä½ æ˜¯èµ„æ·±å¸‚åœºæƒ…ç»ªä¸è¡Œä¸ºé‡‘èå­¦åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯å…³äºæ ‡çš„ä»£ç  {ticker} çš„è¿‘æœŸç‰¹å®šæ–°é—»ï¼Œä»¥åŠå½“å‰å…¨å¸‚åœºçš„å®è§‚èµ„è®¯å¿«æŠ¥ï¼š
    {n_data}

    è¯·æ‰§è¡Œä»¥ä¸‹â€œæƒ…ç»ªè¿‡æ»¤åè®®â€ï¼š
    1. è¯„ä¼°å¾®è§‚äº‹ä»¶çš„é‡çº§ï¼šè¯¥æ ‡çš„è‡ªèº«çš„æ–°é—»æ˜¯å®è´¨æ€§åˆ©å¥½/åˆ©ç©ºï¼Œè¿˜æ˜¯å™ªéŸ³ï¼Ÿ
    2. ç»“åˆå®è§‚èƒŒæ™¯ï¼šå½“å‰å¤§ç›˜æƒ…ç»ªï¼ˆæ ¹æ®å¿«è®¯åˆ¤æ–­ï¼‰æ˜¯åœ¨é…åˆè¯¥æ ‡çš„ä¸Šæ¶¨ï¼Œè¿˜æ˜¯å‹åˆ¶è¯¥æ ‡çš„ï¼Ÿ
    3. è¯„ä¼°æ•£æˆ·/ä¸»åŠ›çš„åšå¼ˆçŠ¶æ€ï¼ˆè´ªå©ª vs ææ…Œï¼‰ã€‚

    è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
    ã€æƒ…ç»ªæŒ‡æ•°ã€‘è´ªå©ª / ææ…Œ / ä¸­æ€§åˆ†åŒ–
    ã€æ ¸å¿ƒé©±åŠ¨åŠ›ã€‘(æŒ‡å‡ºæ˜¯å“ªæ¡å¾®è§‚æ–°é—»æˆ–å®è§‚é€»è¾‘åœ¨ä¸»å¯¼)
    ã€æƒ…ç»ªä¸å¤§ç›˜å…±æŒ¯ã€‘(è¯¥è‚¡ç¥¨å½“å‰æƒ…ç»ªæ˜¯é¡ºåº”å¤§ç›˜è¿˜æ˜¯é€†åŠ¿åšå¼ˆï¼Ÿ)
    """

    response = llm.invoke(prompt)

    return {
        "news_data": n_data,
        "sentiment_signal": response.content,
        "news_links": raw_news_list  # æ–°å¢ï¼šæŠŠå¸¦æœ‰ URL çš„æ–°é—»åˆ—è¡¨ä¼ é€’å‡ºå»
    }


# ==========================================
# 5. ç‹¬ç«‹æµ‹è¯•å…¥å£
# ==========================================
if __name__ == "__main__":
    test_ticker = "sh.600519"  # æµ‹è¯•æ ‡çš„ï¼šè´µå·èŒ…å°
    print(f"ğŸš€ å¯åŠ¨ã€å…¨ç½‘ç‰ˆã€‘å¸‚åœºæƒ…ç»ªæ™ºèƒ½ä½“ç‹¬ç«‹æµ‹è¯• (ç›®æ ‡: {test_ticker})")
    print("-" * 50)

    result = run_sentiment_agent(test_ticker, "sk-xxxxxxè¿™é‡Œæ¢æˆä½ çš„çœŸå®key")

    print("\n" + "=" * 50)
    print("ğŸŒ æœé›†åˆ°çš„åŸå§‹èˆ†æƒ…ä¸çˆ¬è™«èšåˆæ•°æ®:")
    print(result["news_data"])
    print("\n" + "=" * 50)
    print("ğŸ§  æ™ºèƒ½ä½“æƒ…ç»ªåˆ†ææŠ¥å‘Š:")
    print(result["sentiment_signal"])
    print("=" * 50)